import { NextRequest, NextResponse } from 'next/server'
import speech from '@google-cloud/speech'

// Google Cloud Speech-to-Text client
let speechClient: speech.SpeechClient | null = null

function getSpeechClient() {
  if (!speechClient) {
    // æœ¬ç•ªç’°å¢ƒ: ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®JSONæ–‡å­—åˆ—ã‚’èª­ã¿è¾¼ã‚€ï¼ˆVercel/Cloud Runï¼‰
    // é–‹ç™ºç’°å¢ƒ: gcloud auth application-default login ã§èªè¨¼
    if (process.env.GOOGLE_CLOUD_SERVICE_ACCOUNT_JSON) {
      const credentials = JSON.parse(process.env.GOOGLE_CLOUD_SERVICE_ACCOUNT_JSON)
      speechClient = new speech.SpeechClient({
        credentials,
        projectId: process.env.GOOGLE_CLOUD_PROJECT_ID,
      })
    } else {
      // é–‹ç™ºç’°å¢ƒ: gcloud auth application-default login
      // ã¾ãŸã¯ Cloud Run/GKE/GCE ã§ã¯è‡ªå‹•çš„ã«ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’ä½¿ç”¨
      const projectId = process.env.GOOGLE_CLOUD_PROJECT_ID
      speechClient = new speech.SpeechClient({
        projectId,
        // ADCã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã‚‚æ˜ç¤ºçš„ã«quotaProjectIdã‚’è¨­å®š
        ...(projectId && { quotaProjectId: projectId })
      })
    }
  }
  return speechClient
}

export async function POST(request: NextRequest) {
  try {
    const formData = await request.formData()
    const audioFile = formData.get('audio') as File
    const vocabulary = formData.get('vocabulary') as string | null

    if (!audioFile) {
      return NextResponse.json(
        { error: 'éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“' },
        { status: 400 }
      )
    }

    // éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒ•ã‚¡ã«å¤‰æ›
    const audioBytes = await audioFile.arrayBuffer()
    const audioBuffer = Buffer.from(audioBytes)

    console.log('ğŸ¤ å—ä¿¡ã—ãŸéŸ³å£°ãƒ‡ãƒ¼ã‚¿:', {
      size: audioBuffer.length,
      filename: audioFile.name,
      type: audioFile.type,
    })

    const client = getSpeechClient()

    // ã‚«ã‚¹ã‚¿ãƒ èªå½™ã®è¨­å®š
    const customPhrases = vocabulary ? JSON.parse(vocabulary) : []

    // Pæ¤œæŸ»å°‚ç”¨ã®èªå½™ã‚’è¿½åŠ ï¼ˆæ‹¡å¼µç‰ˆï¼‰
    const periodontalPhrases = [
      // æ­¯ç§‘å°‚é–€ç”¨èª
      'é å¿ƒ', 'ä¸­å¤®', 'è¿‘å¿ƒ',
      'é ¬å´', 'èˆŒå´', 'å£è“‹å´',
      'ãˆã‚“ã—ã‚“', 'ã¡ã‚…ã†ãŠã†', 'ãã‚“ã—ã‚“',
      'ãã‚‡ã†ãã', 'ã»ãŠãŒã‚', 'ãœã£ãã', 'ã—ãŸãŒã‚', 'ã“ã†ãŒã„ãã',
      'BOP', 'ãƒ“ãƒ¼ã‚ªãƒ¼ãƒ”ãƒ¼', 'å‡ºè¡€',
      'æ’è†¿', 'ã¯ã„ã®ã†',
      'å‹•æºåº¦', 'ã©ã†ã‚ˆã†ã©',
      'ã‚¹ã‚­ãƒƒãƒ—', 'æ¬ æ',
      // ä½ç½®ã®çµ„ã¿åˆã‚ã›ï¼ˆã‚ˆãä½¿ã‚ã‚Œã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
      'è¿‘å¿ƒé ¬å´', 'ä¸­å¤®é ¬å´', 'é å¿ƒé ¬å´',
      'è¿‘å¿ƒèˆŒå´', 'ä¸­å¤®èˆŒå´', 'é å¿ƒèˆŒå´',
      'è¿‘å¿ƒå£è“‹å´', 'ä¸­å¤®å£è“‹å´', 'é å¿ƒå£è“‹å´',
      // æ­¯ç•ªå· (FDIè¡¨è¨˜)
      ...Array.from({ length: 8 }, (_, i) => `${i + 11}`),
      ...Array.from({ length: 8 }, (_, i) => `${i + 21}`),
      ...Array.from({ length: 8 }, (_, i) => `${i + 31}`),
      ...Array.from({ length: 8 }, (_, i) => `${i + 41}`),
      // æ•°å€¤ï¼ˆ0-15ã¾ã§å…¨ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
      '0', 'ã‚¼ãƒ­', 'ãœã‚', 'é›¶',
      '1', 'ã„ã¡', 'ã‚¤ãƒ', 'ä¸€',
      '2', 'ã«', 'ãƒ‹', 'äºŒ',
      '3', 'ã•ã‚“', 'ã‚µãƒ³', 'ä¸‰',
      '4', 'ã‚ˆã‚“', 'ãƒ¨ãƒ³', 'ã—', 'ã‚·', 'å››',
      '5', 'ã”', 'ã‚´', 'äº”',
      '6', 'ã‚ã', 'ãƒ­ã‚¯', 'å…­',
      '7', 'ãªãª', 'ãƒŠãƒŠ', 'ã—ã¡', 'ã‚·ãƒ', 'ä¸ƒ',
      '8', 'ã¯ã¡', 'ãƒãƒ', 'å…«',
      '9', 'ãã‚…ã†', 'ã‚­ãƒ¥ã‚¦', 'ã', 'ã‚¯', 'ä¹',
      '10', 'ã˜ã‚…ã†', 'ã‚¸ãƒ¥ã‚¦', 'å',
      '11', 'ã˜ã‚…ã†ã„ã¡', 'ã‚¸ãƒ¥ã‚¦ã‚¤ãƒ', 'åä¸€',
      '12', 'ã˜ã‚…ã†ã«', 'ã‚¸ãƒ¥ã‚¦ãƒ‹', 'åäºŒ',
      '13', 'ã˜ã‚…ã†ã•ã‚“', 'ã‚¸ãƒ¥ã‚¦ã‚µãƒ³', 'åä¸‰',
      '14', 'ã˜ã‚…ã†ã‚ˆã‚“', 'ã‚¸ãƒ¥ã‚¦ãƒ¨ãƒ³', 'åå››',
      '15', 'ã˜ã‚…ã†ã”', 'ã‚¸ãƒ¥ã‚¦ã‚´', 'åäº”',
      // ã‚³ãƒãƒ³ãƒ‰
      'ãƒã‚±ãƒƒãƒˆ', 'PPD', 'ãƒ”ãƒ¼ãƒ”ãƒ¼ãƒ‡ã‚£ãƒ¼',
      'æˆ»ã‚‹', 'ã‚‚ã©ã‚‹', 'è¨‚æ­£',
      // åŒºåˆ‡ã‚Šã¨ã—ã¦ä½¿ã‚ã‚Œã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹è¨€è‘‰
      'æ¬¡', 'ã¤ã', 'ç¶šã', 'ã¤ã¥ã'
    ]

    const allPhrases = [...new Set([...periodontalPhrases, ...customPhrases])]

    // éŸ³å£°ã®æ¨å®šé•·ã•ã‚’è¨ˆç®—ï¼ˆWebM/Opus 48kHz mono â‰ˆ 4KB/sï¼‰
    const estimatedDurationSec = audioBuffer.length / 4000
    const isLongAudio = estimatedDurationSec > 50 // 50ç§’ä»¥ä¸Šã¯é•·æ™‚é–“èªè­˜APIã‚’ä½¿ç”¨

    console.log(`â±ï¸ æ¨å®šéŸ³å£°é•·: ${Math.round(estimatedDurationSec)}ç§’ â†’ ${isLongAudio ? 'longRunningRecognize' : 'recognize'} ã‚’ä½¿ç”¨`)

    // èªè­˜è¨­å®šï¼ˆé•·ã„éŸ³å£°ã¯ latest_longã€çŸ­ã„éŸ³å£°ã¯ latest_shortï¼‰
    const config: speech.protos.google.cloud.speech.v1.IRecognitionConfig = {
      encoding: 'WEBM_OPUS',
      sampleRateHertz: 48000,
      languageCode: 'ja-JP',
      enableAutomaticPunctuation: true,
      model: isLongAudio ? 'latest_long' : 'latest_short',
      useEnhanced: true,
      maxAlternatives: 3,
      enableWordConfidence: true,
      speechContexts: allPhrases.length > 0 ? [
        {
          phrases: allPhrases.slice(0, 500),
          boost: 15,
        },
      ] : [],
      profanityFilter: false,
    }

    const audio = {
      content: audioBuffer.toString('base64'),
    }

    const recognitionRequest = {
      config,
      audio,
    }

    // éŸ³å£°èªè­˜ã‚’å®Ÿè¡Œ
    let results: any[] | null | undefined
    let totalBilledTime: any
    let requestId: any

    if (isLongAudio) {
      // 60ç§’è¶…ã®éŸ³å£°: éåŒæœŸAPIï¼ˆlongRunningRecognizeï¼‰ã‚’ä½¿ç”¨
      console.log('ğŸ“¡ longRunningRecognize APIå‘¼ã³å‡ºã—é–‹å§‹...')
      const [operation] = await client.longRunningRecognize(recognitionRequest)
      console.log('â³ å‡¦ç†ä¸­... operationã‚’å¾…æ©Ÿ')
      const [longResponse] = await operation.promise()
      results = longResponse.results
      totalBilledTime = longResponse.totalBilledTime
      requestId = longResponse.requestId
      console.log('âœ… longRunningRecognizeå®Œäº†')
    } else {
      // 60ç§’ä»¥ä¸‹ã®éŸ³å£°: åŒæœŸAPIï¼ˆrecognizeï¼‰ã‚’ä½¿ç”¨
      console.log('ğŸ“¡ recognize APIå‘¼ã³å‡ºã—é–‹å§‹...')
      const [shortResponse] = await client.recognize(recognitionRequest)
      results = shortResponse.results
      totalBilledTime = shortResponse.totalBilledTime
      requestId = shortResponse.requestId
      console.log('âœ… recognizeå®Œäº†')
    }

    if (!results || results.length === 0) {
      console.warn('âš ï¸ èªè­˜çµæœãŒç©ºã§ã™ï¼ˆç„¡éŸ³ã¾ãŸã¯èªè­˜ä¸å¯èƒ½ãªéŸ³å£°ï¼‰')
      return NextResponse.json({
        transcript: '',
        confidence: 0,
        languageCode: 'ja-JP',
        alternatives: [],
        debug: {
          audioSize: audioBuffer.length,
          estimatedDurationSec: Math.round(estimatedDurationSec),
          resultsCount: 0,
          totalBilledTime,
          requestId,
        }
      })
    }

    console.log('ğŸ¯ èªè­˜æˆåŠŸ:', results.length, 'å€‹ã®çµæœ')

    const transcription = results
      .map(result => result.alternatives?.[0]?.transcript || '')
      .join(' ')
      .trim()

    const confidence = results[0]?.alternatives?.[0]?.confidence || 0

    // ä»£æ›¿å€™è£œã‚‚è¿”ã™
    const alternatives = results[0]?.alternatives?.slice(1, 4).map(alt => ({
      transcript: alt.transcript || '',
      confidence: alt.confidence || 0,
    })) || []

    return NextResponse.json({
      transcript: transcription,
      confidence,
      languageCode: 'ja-JP',
      alternatives,
    })
  } catch (error) {
    console.error('Speech-to-Text API ã‚¨ãƒ©ãƒ¼:', error)
    return NextResponse.json(
      {
        error: 'éŸ³å£°èªè­˜ã«å¤±æ•—ã—ã¾ã—ãŸ',
        details: error instanceof Error ? error.message : 'Unknown error'
      },
      { status: 500 }
    )
  }
}
